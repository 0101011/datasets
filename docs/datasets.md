<!-- auto-generated by tfds.scripts.document_datasets -->
# Datasets

## Usage

```
# See all registered datasets
tfds.list_builders()

# Load a given dataset by name, along with the DatasetInfo
data, info = tfds.load("mnist", with_info=True)
train_data, test_data = data['test'], data['train']
assert isinstance(train_data, tf.data.Dataset)
assert info.features['label'].num_classes == 10
assert info.splits['train'].num_examples == 60000

# You can also access a builder directly
builder = tfds.builder("mnist")
assert builder.info.splits['train'].num_examples == 60000
builder.download_and_prepare()
datasets = builder.as_dataset()

# If you need NumPy arrays
np_datasets = tfds.dataset_as_numpy(datasets)
```

---

# Datasets

* [`audio`](#audio)
  * [`"librispeech"`](#librispeech)
* [`image`](#image)
  * [`"celeb_a"`](#celeb_a)
  * [`"cifar10"`](#cifar10)
  * [`"cifar100"`](#cifar100)
  * [`"coco2014"`](#coco2014)
  * [`"diabetic_retinopathy_detection"`](#diabetic_retinopathy_detection)
  * [`"fashion_mnist"`](#fashion_mnist)
  * [`"image_label_folder"`](#image_label_folder)
  * [`"imagenet2012"`](#imagenet2012)
  * [`"lsun"`](#lsun)
  * [`"mnist"`](#mnist)
  * [`"omniglot"`](#omniglot)
  * [`"svhn_cropped"`](#svhn_cropped)
* [`text`](#text)
  * [`"imdb_reviews"`](#imdb_reviews)
  * [`"lm1b"`](#lm1b)
  * [`"squad"`](#squad)
* [`translate`](#translate)
  * [`"wmt_translate_ende"`](#wmt_translate_ende)
  * [`"wmt_translate_enfr"`](#wmt_translate_enfr)
* [`video`](#video)
  * [`"bair_robot_pushing_small"`](#bair_robot_pushing_small)
  * [`"starcraft_video"`](#starcraft_video)

---

# [`audio`](#audio)

## `"librispeech"`

LibriSpeech is a corpus of approximately 1000 hours of 16kHz read English speech, prepared by Vassil Panayotov with the assistance of Daniel Povey. The data is derived from read audiobooks from the LibriVox project, and has been carefully segmented and aligned.


* URL: http://www.openslr.org/12
* `DatasetBuilder`: [`tfds.audio.librispeech.Librispeech`](https://github.com/tensorflow/datasets/tree/master/tensorflow_datasets/audio/librispeech.py)

`librispeech` is configured with `tfds.audio.librispeech.LibrispeechConfig` and has the following
configurations predefined (defaults to the first one):

* `"clean100_plain_text"` (`v0.1.0`): Uses only clean data, not including train-clean-360. Transcriptions are in plain text.

* `"clean360_plain_text"` (`v0.1.0`): Uses only clean data, including train-clean-360. Transcriptions are in plain text.

* `"all_plain_text"` (`v0.1.0`): Uses all data. Transcriptions are in plain text.

* `"clean100_bytes"` (`v0.1.0`): Uses only clean data, not including train-clean-360. Transcriptions use the ByteTextEncoder

* `"clean360_bytes"` (`v0.1.0`): Uses only clean data, including train-clean-360. Transcriptions use the ByteTextEncoder

* `"all_bytes"` (`v0.1.0`): Uses all data. Transcriptions use the ByteTextEncoder

* `"clean100_subwords8k"` (`v0.1.0`): Uses only clean data, not including train-clean-360. Transcriptions use the SubwordTextEncoder

* `"clean360_subwords8k"` (`v0.1.0`): Uses only clean data, including train-clean-360. Transcriptions use the SubwordTextEncoder

* `"all_subwords8k"` (`v0.1.0`): Uses all data. Transcriptions use the SubwordTextEncoder

* `"clean100_subwords32k"` (`v0.1.0`): Uses only clean data, not including train-clean-360. Transcriptions use the SubwordTextEncoder

* `"clean360_subwords32k"` (`v0.1.0`): Uses only clean data, including train-clean-360. Transcriptions use the SubwordTextEncoder

* `"all_subwords32k"` (`v0.1.0`): Uses all data. Transcriptions use the SubwordTextEncoder


### `"librispeech/clean100_plain_text"`

```
FeaturesDict({
    'chapter_id': Tensor(shape=(), dtype=tf.int64),
    'speaker_id': Tensor(shape=(), dtype=tf.int64),
    'speech': Audio(shape=(None,), dtype=tf.int64),
    'text': Text(shape=(), dtype=tf.string, encoder=None),
})
```



### `"librispeech/clean360_plain_text"`

```
FeaturesDict({
    'chapter_id': Tensor(shape=(), dtype=tf.int64),
    'speaker_id': Tensor(shape=(), dtype=tf.int64),
    'speech': Audio(shape=(None,), dtype=tf.int64),
    'text': Text(shape=(), dtype=tf.string, encoder=None),
})
```



### `"librispeech/all_plain_text"`

```
FeaturesDict({
    'chapter_id': Tensor(shape=(), dtype=tf.int64),
    'speaker_id': Tensor(shape=(), dtype=tf.int64),
    'speech': Audio(shape=(None,), dtype=tf.int64),
    'text': Text(shape=(), dtype=tf.string, encoder=None),
})
```



### `"librispeech/clean100_bytes"`

```
FeaturesDict({
    'chapter_id': Tensor(shape=(), dtype=tf.int64),
    'speaker_id': Tensor(shape=(), dtype=tf.int64),
    'speech': Audio(shape=(None,), dtype=tf.int64),
    'text': Text(shape=(None,), dtype=tf.int64, encoder=<ByteTextEncoder vocab_size=257>),
})
```



### `"librispeech/clean360_bytes"`

```
FeaturesDict({
    'chapter_id': Tensor(shape=(), dtype=tf.int64),
    'speaker_id': Tensor(shape=(), dtype=tf.int64),
    'speech': Audio(shape=(None,), dtype=tf.int64),
    'text': Text(shape=(None,), dtype=tf.int64, encoder=<ByteTextEncoder vocab_size=257>),
})
```



### `"librispeech/all_bytes"`

```
FeaturesDict({
    'chapter_id': Tensor(shape=(), dtype=tf.int64),
    'speaker_id': Tensor(shape=(), dtype=tf.int64),
    'speech': Audio(shape=(None,), dtype=tf.int64),
    'text': Text(shape=(None,), dtype=tf.int64, encoder=<ByteTextEncoder vocab_size=257>),
})
```



### `"librispeech/clean100_subwords8k"`

```
FeaturesDict({
    'chapter_id': Tensor(shape=(), dtype=tf.int64),
    'speaker_id': Tensor(shape=(), dtype=tf.int64),
    'speech': Audio(shape=(None,), dtype=tf.int64),
    'text': Text(shape=(), dtype=tf.string, encoder=None),
})
```



### `"librispeech/clean360_subwords8k"`

```
FeaturesDict({
    'chapter_id': Tensor(shape=(), dtype=tf.int64),
    'speaker_id': Tensor(shape=(), dtype=tf.int64),
    'speech': Audio(shape=(None,), dtype=tf.int64),
    'text': Text(shape=(), dtype=tf.string, encoder=None),
})
```



### `"librispeech/all_subwords8k"`

```
FeaturesDict({
    'chapter_id': Tensor(shape=(), dtype=tf.int64),
    'speaker_id': Tensor(shape=(), dtype=tf.int64),
    'speech': Audio(shape=(None,), dtype=tf.int64),
    'text': Text(shape=(), dtype=tf.string, encoder=None),
})
```



### `"librispeech/clean100_subwords32k"`

```
FeaturesDict({
    'chapter_id': Tensor(shape=(), dtype=tf.int64),
    'speaker_id': Tensor(shape=(), dtype=tf.int64),
    'speech': Audio(shape=(None,), dtype=tf.int64),
    'text': Text(shape=(), dtype=tf.string, encoder=None),
})
```



### `"librispeech/clean360_subwords32k"`

```
FeaturesDict({
    'chapter_id': Tensor(shape=(), dtype=tf.int64),
    'speaker_id': Tensor(shape=(), dtype=tf.int64),
    'speech': Audio(shape=(None,), dtype=tf.int64),
    'text': Text(shape=(), dtype=tf.string, encoder=None),
})
```



### `"librispeech/all_subwords32k"`

```
FeaturesDict({
    'chapter_id': Tensor(shape=(), dtype=tf.int64),
    'speaker_id': Tensor(shape=(), dtype=tf.int64),
    'speech': Audio(shape=(None,), dtype=tf.int64),
    'text': Text(shape=(), dtype=tf.string, encoder=None),
})
```




### Statistics
None computed

### Urls
 * http://www.openslr.org/12

### Supervised keys (for `as_supervised=True`)
(u'speech', u'text')

### Citation
```
@inproceedings{panayotov2015librispeech,
  title={Librispeech: an ASR corpus based on public domain audio books},
  author={Panayotov, Vassil and Chen, Guoguo and Povey, Daniel and Khudanpur, Sanjeev},
  booktitle={Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on},
  pages={5206--5210},
  year={2015},
  organization={IEEE}
}

```

---


# [`image`](#image)

## `"celeb_a"`

Large-scale CelebFaces Attributes, CelebA.Set of ~30k celebrities pictures. These pictures are cropped.

* URL: http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html
* `DatasetBuilder`: [`tfds.image.celeba.CelebA`](https://github.com/tensorflow/datasets/tree/master/tensorflow_datasets/image/celeba.py)
* Version: `v0.2.0`

### Features
```
FeaturesDict({
    'attributes': FeaturesDict({
        '5_o_Clock_Shadow': Tensor(shape=(), dtype=tf.bool),
        'Arched_Eyebrows': Tensor(shape=(), dtype=tf.bool),
        'Attractive': Tensor(shape=(), dtype=tf.bool),
        'Bags_Under_Eyes': Tensor(shape=(), dtype=tf.bool),
        'Bald': Tensor(shape=(), dtype=tf.bool),
        'Bangs': Tensor(shape=(), dtype=tf.bool),
        'Big_Lips': Tensor(shape=(), dtype=tf.bool),
        'Big_Nose': Tensor(shape=(), dtype=tf.bool),
        'Black_Hair': Tensor(shape=(), dtype=tf.bool),
        'Blond_Hair': Tensor(shape=(), dtype=tf.bool),
        'Blurry': Tensor(shape=(), dtype=tf.bool),
        'Brown_Hair': Tensor(shape=(), dtype=tf.bool),
        'Bushy_Eyebrows': Tensor(shape=(), dtype=tf.bool),
        'Chubby': Tensor(shape=(), dtype=tf.bool),
        'Double_Chin': Tensor(shape=(), dtype=tf.bool),
        'Eyeglasses': Tensor(shape=(), dtype=tf.bool),
        'Goatee': Tensor(shape=(), dtype=tf.bool),
        'Gray_Hair': Tensor(shape=(), dtype=tf.bool),
        'Heavy_Makeup': Tensor(shape=(), dtype=tf.bool),
        'High_Cheekbones': Tensor(shape=(), dtype=tf.bool),
        'Male': Tensor(shape=(), dtype=tf.bool),
        'Mouth_Slightly_Open': Tensor(shape=(), dtype=tf.bool),
        'Mustache': Tensor(shape=(), dtype=tf.bool),
        'Narrow_Eyes': Tensor(shape=(), dtype=tf.bool),
        'No_Beard': Tensor(shape=(), dtype=tf.bool),
        'Oval_Face': Tensor(shape=(), dtype=tf.bool),
        'Pale_Skin': Tensor(shape=(), dtype=tf.bool),
        'Pointy_Nose': Tensor(shape=(), dtype=tf.bool),
        'Receding_Hairline': Tensor(shape=(), dtype=tf.bool),
        'Rosy_Cheeks': Tensor(shape=(), dtype=tf.bool),
        'Sideburns': Tensor(shape=(), dtype=tf.bool),
        'Smiling': Tensor(shape=(), dtype=tf.bool),
        'Straight_Hair': Tensor(shape=(), dtype=tf.bool),
        'Wavy_Hair': Tensor(shape=(), dtype=tf.bool),
        'Wearing_Earrings': Tensor(shape=(), dtype=tf.bool),
        'Wearing_Hat': Tensor(shape=(), dtype=tf.bool),
        'Wearing_Lipstick': Tensor(shape=(), dtype=tf.bool),
        'Wearing_Necklace': Tensor(shape=(), dtype=tf.bool),
        'Wearing_Necktie': Tensor(shape=(), dtype=tf.bool),
        'Young': Tensor(shape=(), dtype=tf.bool),
    }),
    'image': Image(shape=(218, 178, 3), dtype=tf.uint8),
    'landmarks': FeaturesDict({
        'lefteye_x': Tensor(shape=(), dtype=tf.int64),
        'lefteye_y': Tensor(shape=(), dtype=tf.int64),
        'leftmouth_x': Tensor(shape=(), dtype=tf.int64),
        'leftmouth_y': Tensor(shape=(), dtype=tf.int64),
        'nose_x': Tensor(shape=(), dtype=tf.int64),
        'nose_y': Tensor(shape=(), dtype=tf.int64),
        'righteye_x': Tensor(shape=(), dtype=tf.int64),
        'righteye_y': Tensor(shape=(), dtype=tf.int64),
        'rightmouth_x': Tensor(shape=(), dtype=tf.int64),
        'rightmouth_y': Tensor(shape=(), dtype=tf.int64),
    }),
})
```


### Statistics
Split  | Examples
:----- | ---:
ALL        |    202,599
TRAIN      |    162,770
TEST       |     19,962
VALIDATION |     19,867


### Urls
 * http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html

### Supervised keys (for `as_supervised=True`)
None

### Citation
```
@inproceedings{conf/iccv/LiuLWT15,
  added-at = {2018-10-09T00:00:00.000+0200},
  author = {Liu, Ziwei and Luo, Ping and Wang, Xiaogang and Tang, Xiaoou},
  biburl = {https://www.bibsonomy.org/bibtex/250e4959be61db325d2f02c1d8cd7bfbb/dblp},
  booktitle = {ICCV},
  crossref = {conf/iccv/2015},
  ee = {http://doi.ieeecomputersociety.org/10.1109/ICCV.2015.425},
  interhash = {3f735aaa11957e73914bbe2ca9d5e702},
  intrahash = {50e4959be61db325d2f02c1d8cd7bfbb},
  isbn = {978-1-4673-8391-2},
  keywords = {dblp},
  pages = {3730-3738},
  publisher = {IEEE Computer Society},
  timestamp = {2018-10-11T11:43:28.000+0200},
  title = {Deep Learning Face Attributes in the Wild.},
  url = {http://dblp.uni-trier.de/db/conf/iccv/iccv2015.html#LiuLWT15},
  year = 2015
}

```

---

## `"cifar10"`

The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.

* URL: https://www.cs.toronto.edu/~kriz/cifar.html
* `DatasetBuilder`: [`tfds.image.cifar.Cifar10`](https://github.com/tensorflow/datasets/tree/master/tensorflow_datasets/image/cifar.py)
* Version: `v1.0.1`

### Features
```
FeaturesDict({
    'image': Image(shape=(32, 32, 3), dtype=tf.uint8),
    'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),
})
```


### Statistics
Split  | Examples
:----- | ---:
ALL        |     60,000
TRAIN      |     50,000
TEST       |     10,000


### Urls
 * https://www.cs.toronto.edu/~kriz/cifar.html

### Supervised keys (for `as_supervised=True`)
(u'image', u'label')

### Citation
```
@TECHREPORT{Krizhevsky09learningmultiple,
    author = {Alex Krizhevsky},
    title = {Learning multiple layers of features from tiny images},
    institution = {},
    year = {2009}
}

```

---

## `"cifar100"`

This dataset is just like the CIFAR-10, except it has 100 classes containing 600 images each. There are 500 training images and 100 testing images per class. The 100 classes in the CIFAR-100 are grouped into 20 superclasses. Each image comes with a "fine" label (the class to which it belongs) and a "coarse" label (the superclass to which it belongs).

* URL: https://www.cs.toronto.edu/~kriz/cifar.html
* `DatasetBuilder`: [`tfds.image.cifar.Cifar100`](https://github.com/tensorflow/datasets/tree/master/tensorflow_datasets/image/cifar.py)
* Version: `v1.3.0`

### Features
```
FeaturesDict({
    'coarse_label': ClassLabel(shape=(), dtype=tf.int64, num_classes=20),
    'image': Image(shape=(32, 32, 3), dtype=tf.uint8),
    'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=100),
})
```


### Statistics
Split  | Examples
:----- | ---:
ALL        |     60,000
TRAIN      |     50,000
TEST       |     10,000


### Urls
 * https://www.cs.toronto.edu/~kriz/cifar.html

### Supervised keys (for `as_supervised=True`)
(u'image', u'label')

### Citation
```
@TECHREPORT{Krizhevsky09learningmultiple,
    author = {Alex Krizhevsky},
    title = {Learning multiple layers of features from tiny images},
    institution = {},
    year = {2009}
}

```

---

## `"coco2014"`

COCO is a large-scale object detection, segmentation, and captioning dataset. This version contains images, bounding boxes and labels for the 2014 version.
Note:
 * Some images from the train and validation sets don't have annotations.
 * The test split don't have any annotations (only images).
 * Coco defines 91 classes but the data only had 80 classes.


* URL: http://cocodataset.org/#home
* `DatasetBuilder`: [`tfds.image.coco.Coco2014`](https://github.com/tensorflow/datasets/tree/master/tensorflow_datasets/image/coco.py)
* Version: `v1.0.0`

### Features
```
FeaturesDict({
    'image': Image(shape=(None, None, 3), dtype=tf.uint8),
    'image/filename': Text(shape=(), dtype=tf.string, encoder=None),
    'objects': FeaturesDict({
        'bbox': BBoxFeature(shape=(4,), dtype=tf.float32),
        'is_crowd': Tensor(shape=(), dtype=tf.bool),
        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=80),
    }),
})
```


### Statistics
Split  | Examples
:----- | ---:
ALL        |    245,496
TRAIN      |     82,783
TEST2015   |     81,434
TEST       |     40,775
VALIDATION |     40,504


### Urls
 * http://cocodataset.org/#home

### Supervised keys (for `as_supervised=True`)
None

### Citation
```
@article{DBLP:journals/corr/LinMBHPRDZ14,
  author    = {Tsung{-}Yi Lin and
               Michael Maire and
               Serge J. Belongie and
               Lubomir D. Bourdev and
               Ross B. Girshick and
               James Hays and
               Pietro Perona and
               Deva Ramanan and
               Piotr Doll{'{a}}r and
               C. Lawrence Zitnick},
  title     = {Microsoft {COCO:} Common Objects in Context},
  journal   = {CoRR},
  volume    = {abs/1405.0312},
  year      = {2014},
  url       = {http://arxiv.org/abs/1405.0312},
  archivePrefix = {arXiv},
  eprint    = {1405.0312},
  timestamp = {Mon, 13 Aug 2018 16:48:13 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/LinMBHPRDZ14},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

```

---

## `"diabetic_retinopathy_detection"`

A large set of high-resolution retina images taken under a variety of imaging conditions.

* URL: https://www.kaggle.com/c/diabetic-retinopathy-detection/data
* `DatasetBuilder`: [`tfds.image.diabetic_retinopathy_detection.DiabeticRetinopathyDetection`](https://github.com/tensorflow/datasets/tree/master/tensorflow_datasets/image/diabetic_retinopathy_detection.py)
* Version: `v1.0.0`

### Features
```
FeaturesDict({
    'image': Image(shape=(None, None, 3), dtype=tf.uint8),
    'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=5),
    'name': Text(shape=(), dtype=tf.string, encoder=None),
})
```


### Statistics
Split  | Examples
:----- | ---:
ALL        |     88,712
TEST       |     53,576
TRAIN      |     35,126
SAMPLE     |         10


### Urls
 * https://www.kaggle.com/c/diabetic-retinopathy-detection/data

### Supervised keys (for `as_supervised=True`)
None

### Citation
```

```

---

## `"fashion_mnist"`

Fashion-MNIST is a dataset of Zalando's article images consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes.

* URL: https://github.com/zalandoresearch/fashion-mnist
* `DatasetBuilder`: [`tfds.image.mnist.FashionMNIST`](https://github.com/tensorflow/datasets/tree/master/tensorflow_datasets/image/mnist.py)
* Version: `v1.0.0`

### Features
```
FeaturesDict({
    'image': Image(shape=(28, 28, 1), dtype=tf.uint8),
    'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),
})
```


### Statistics
Split  | Examples
:----- | ---:
ALL        |     70,000
TRAIN      |     60,000
TEST       |     10,000


### Urls
 * https://github.com/zalandoresearch/fashion-mnist

### Supervised keys (for `as_supervised=True`)
(u'image', u'label')

### Citation
```
@article{DBLP:journals/corr/abs-1708-07747,
  author    = {Han Xiao and
               Kashif Rasul and
               Roland Vollgraf},
  title     = {Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning
               Algorithms},
  journal   = {CoRR},
  volume    = {abs/1708.07747},
  year      = {2017},
  url       = {http://arxiv.org/abs/1708.07747},
  archivePrefix = {arXiv},
  eprint    = {1708.07747},
  timestamp = {Mon, 13 Aug 2018 16:47:27 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1708-07747},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

```

---

## `"image_label_folder"`

Generic image classification dataset.

* URL: <no known url>
* `DatasetBuilder`: [`tfds.image.image_folder.ImageLabelFolder`](https://github.com/tensorflow/datasets/tree/master/tensorflow_datasets/image/image_folder.py)
* Version: `v1.0.0`

### Features
```
FeaturesDict({
    'image': Image(shape=(None, None, 3), dtype=tf.uint8),
    'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=None),
})
```


### Statistics
None computed

### Urls


### Supervised keys (for `as_supervised=True`)
(u'image', u'label')

### Citation
```

```

---

## `"imagenet2012"`

ILSVRC 2012, aka ImageNet is an image dataset organized according to the
WordNet hierarchy. Each meaningful concept in WordNet, possibly described by
multiple words or word phrases, is called a "synonym set" or "synset". There are
more than 100,000 synsets in WordNet, majority of them are nouns (80,000+). In
ImageNet, we aim to provide on average 1000 images to illustrate each synset.
Images of each concept are quality-controlled and human-annotated. In its
completion, we hope ImageNet will offer tens of millions of cleanly sorted
images for most of the concepts in the WordNet hierarchy.


* URL: http://image-net.org/
* `DatasetBuilder`: [`tfds.image.imagenet.Imagenet2012`](https://github.com/tensorflow/datasets/tree/master/tensorflow_datasets/image/imagenet.py)
* Version: `v1.0.0`

### Features
```
FeaturesDict({
    'file_name': Text(shape=(), dtype=tf.string, encoder=None),
    'image': Image(shape=(None, None, 3), dtype=tf.uint8),
    'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=1000),
})
```


### Statistics
Split  | Examples
:----- | ---:
ALL        |  1,331,167
TRAIN      |  1,281,167
VALIDATION |     50,000


### Urls
 * http://image-net.org/

### Supervised keys (for `as_supervised=True`)
(u'image', u'label')

### Citation
```
@article{ILSVRC15,
Author = {Olga Russakovsky and Jia Deng and Hao Su and Jonathan Krause and Sanjeev Satheesh and Sean Ma and Zhiheng Huang and Andrej Karpathy and Aditya Khosla and Michael Bernstein and Alexander C. Berg and Li Fei-Fei},
Title = {{ImageNet Large Scale Visual Recognition Challenge}},
Year = {2015},
journal   = {International Journal of Computer Vision (IJCV)},
doi = {10.1007/s11263-015-0816-y},
volume={115},
number={3},
pages={211-252}
}

```

---

## `"lsun"`

Large scale images showing different objects from given categories like bedroom, tower etc.

* URL: https://www.yf.io/p/lsun
* `DatasetBuilder`: [`tfds.image.lsun.Lsun`](https://github.com/tensorflow/datasets/tree/master/tensorflow_datasets/image/lsun.py)

`lsun` is configured with `tfds.image.lsun.BuilderConfig` and has the following
configurations predefined (defaults to the first one):

* `"classroom"` (`v0.1.1`): Classroom images.

* `"bedroom"` (`v0.1.1`): Bedroom images.


### `"lsun/classroom"`

```
FeaturesDict({
    'image': Image(shape=(None, None, 3), dtype=tf.uint8),
})
```



### `"lsun/bedroom"`

```
FeaturesDict({
    'image': Image(shape=(None, None, 3), dtype=tf.uint8),
})
```




### Statistics
None computed

### Urls
 * https://www.yf.io/p/lsun

### Supervised keys (for `as_supervised=True`)
None

### Citation
```
@article{journals/corr/YuZSSX15,
  added-at = {2018-08-13T00:00:00.000+0200},
  author = {Yu, Fisher and Zhang, Yinda and Song, Shuran and Seff, Ari and Xiao, Jianxiong},
  biburl = {https://www.bibsonomy.org/bibtex/2446d4ffb99a5d7d2ab6e5417a12e195f/dblp},
  ee = {http://arxiv.org/abs/1506.03365},
  interhash = {3e9306c4ce2ead125f3b2ab0e25adc85},
  intrahash = {446d4ffb99a5d7d2ab6e5417a12e195f},
  journal = {CoRR},
  keywords = {dblp},
  timestamp = {2018-08-14T15:08:59.000+0200},
  title = {LSUN: Construction of a Large-scale Image Dataset using Deep Learning with Humans in the Loop.},
  url = {http://dblp.uni-trier.de/db/journals/corr/corr1506.html#YuZSSX15},
  volume = {abs/1506.03365},
  year = 2015
}

```

---

## `"mnist"`

The MNIST database of handwritten digits.

* URL: http://yann.lecun.com/exdb/mnist/
* `DatasetBuilder`: [`tfds.image.mnist.MNIST`](https://github.com/tensorflow/datasets/tree/master/tensorflow_datasets/image/mnist.py)
* Version: `v1.0.0`

### Features
```
FeaturesDict({
    'image': Image(shape=(28, 28, 1), dtype=tf.uint8),
    'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),
})
```


### Statistics
Split  | Examples
:----- | ---:
ALL        |     70,000
TRAIN      |     60,000
TEST       |     10,000


### Urls
 * http://yann.lecun.com/exdb/mnist/

### Supervised keys (for `as_supervised=True`)
(u'image', u'label')

### Citation
```
@article{lecun2010mnist,
  title={MNIST handwritten digit database},
  author={LeCun, Yann and Cortes, Corinna and Burges, CJ},
  journal={ATT Labs [Online]. Available: http://yann. lecun. com/exdb/mnist},
  volume={2},
  year={2010}
}

```

---

## `"omniglot"`

Omniglot data set for one-shot learning. This dataset contains 1623 different
handwritten characters from 50 different alphabets.


* URL: https://github.com/brendenlake/omniglot/
* `DatasetBuilder`: [`tfds.image.omniglot.Omniglot`](https://github.com/tensorflow/datasets/tree/master/tensorflow_datasets/image/omniglot.py)
* Version: `v1.0.0`

### Features
```
FeaturesDict({
    'alphabet': ClassLabel(shape=(), dtype=tf.int64, num_classes=50),
    'alphabet_char_id': Tensor(shape=(), dtype=tf.int64),
    'image': Image(shape=(105, 105, 3), dtype=tf.uint8),
    'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=1623),
})
```


### Statistics
Split  | Examples
:----- | ---:
ALL        |     38,300
TRAIN      |     19,280
TEST       |     13,180
SMALL2     |      3,120
SMALL1     |      2,720


### Urls
 * https://github.com/brendenlake/omniglot/

### Supervised keys (for `as_supervised=True`)
(u'image', u'label')

### Citation
```
@article{lake2015human,
  title={Human-level concept learning through probabilistic program induction},
  author={Lake, Brenden M and Salakhutdinov, Ruslan and Tenenbaum, Joshua B},
  journal={Science},
  volume={350},
  number={6266},
  pages={1332--1338},
  year={2015},
  publisher={American Association for the Advancement of Science}
}

```

---

## `"svhn_cropped"`

The Street View House Numbers (SVHN) Dataset is an image digit recognition dataset of over 600,000 digit images coming from real world data. Images are cropped to 32x32.

* URL: http://ufldl.stanford.edu/housenumbers/
* `DatasetBuilder`: [`tfds.image.svhn.SvhnCropped`](https://github.com/tensorflow/datasets/tree/master/tensorflow_datasets/image/svhn.py)
* Version: `v1.0.0`

### Features
```
FeaturesDict({
    'image': Image(shape=(32, 32, 3), dtype=tf.uint8),
    'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),
})
```


### Statistics
Split  | Examples
:----- | ---:
ALL        |    630,420
EXTRA      |    531,131
TRAIN      |     73,257
TEST       |     26,032


### Urls
 * http://ufldl.stanford.edu/housenumbers/

### Supervised keys (for `as_supervised=True`)
(u'image', u'label')

### Citation
```
@article{Netzer2011,
author = {Netzer, Yuval and Wang, Tao and Coates, Adam and Bissacco, Alessandro and Wu, Bo and Ng, Andrew Y},
booktitle = {Advances in Neural Information Processing Systems ({NIPS})},
title = {Reading Digits in Natural Images with Unsupervised Feature Learning},
year = {2011}
}

```

---


# [`text`](#text)

## `"imdb_reviews"`

Large Movie Review Dataset.
This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing.

* URL: http://ai.stanford.edu/~amaas/data/sentiment/
* `DatasetBuilder`: [`tfds.text.imdb.IMDBReviews`](https://github.com/tensorflow/datasets/tree/master/tensorflow_datasets/text/imdb.py)

`imdb_reviews` is configured with `tfds.text.imdb.IMDBReviewsConfig` and has the following
configurations predefined (defaults to the first one):

* `"plain_text"` (`v0.0.1`): Plain text

* `"bytes"` (`v0.0.1`): Uses byte-level text encoding with `tfds.features.text.ByteTextEncoder`

* `"subwords8k"` (`v0.0.1`): Uses `tfds.features.text.SubwordTextEncoder` with 8k vocab size

* `"subwords32k"` (`v0.0.1`): Uses `tfds.features.text.SubwordTextEncoder` with 32k vocab size


### `"imdb_reviews/plain_text"`

```
FeaturesDict({
    'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=2),
    'text': Text(shape=(), dtype=tf.string, encoder=None),
})
```



### `"imdb_reviews/bytes"`

```
FeaturesDict({
    'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=2),
    'text': Text(shape=(None,), dtype=tf.int64, encoder=<ByteTextEncoder vocab_size=257>),
})
```



### `"imdb_reviews/subwords8k"`

```
FeaturesDict({
    'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=2),
    'text': Text(shape=(None,), dtype=tf.int64, encoder=<SubwordTextEncoder vocab_size=8185>),
})
```



### `"imdb_reviews/subwords32k"`

```
FeaturesDict({
    'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=2),
    'text': Text(shape=(None,), dtype=tf.int64, encoder=<SubwordTextEncoder vocab_size=32650>),
})
```




### Statistics
Split  | Examples
:----- | ---:
ALL        |     50,000
TRAIN      |     25,000
TEST       |     25,000


### Urls
 * http://ai.stanford.edu/~amaas/data/sentiment/

### Supervised keys (for `as_supervised=True`)
(u'text', u'label')

### Citation
```
@InProceedings{maas-EtAl:2011:ACL-HLT2011,
  author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},
  title     = {Learning Word Vectors for Sentiment Analysis},
  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},
  month     = {June},
  year      = {2011},
  address   = {Portland, Oregon, USA},
  publisher = {Association for Computational Linguistics},
  pages     = {142--150},
  url       = {http://www.aclweb.org/anthology/P11-1015}
}

```

---

## `"lm1b"`

A benchmark corpus to be used for measuring progress in statistical language modeling. This has almost one billion words in the training data.


* URL: http://www.statmt.org/lm-benchmark/
* `DatasetBuilder`: [`tfds.text.lm1b.Lm1b`](https://github.com/tensorflow/datasets/tree/master/tensorflow_datasets/text/lm1b.py)

`lm1b` is configured with `tfds.text.lm1b.Lm1bConfig` and has the following
configurations predefined (defaults to the first one):

* `"plain_text"` (`v0.0.1`): Plain text

* `"bytes"` (`v0.0.1`): Uses byte-level text encoding with `tfds.features.text.ByteTextEncoder`

* `"subwords8k"` (`v0.0.1`): Uses `tfds.features.text.SubwordTextEncoder` with 8k vocab size

* `"subwords32k"` (`v0.0.1`): Uses `tfds.features.text.SubwordTextEncoder` with 32k vocab size


### `"lm1b/plain_text"`

```
FeaturesDict({
    'text': Text(shape=(), dtype=tf.string, encoder=None),
})
```



### `"lm1b/bytes"`

```
FeaturesDict({
    'text': Text(shape=(None,), dtype=tf.int64, encoder=<ByteTextEncoder vocab_size=257>),
})
```



### `"lm1b/subwords8k"`

```
FeaturesDict({
    'text': Text(shape=(None,), dtype=tf.int64, encoder=<SubwordTextEncoder vocab_size=58517>),
})
```



### `"lm1b/subwords32k"`

```
FeaturesDict({
    'text': Text(shape=(None,), dtype=tf.int64, encoder=<SubwordTextEncoder vocab_size=58517>),
})
```




### Statistics
Split  | Examples
:----- | ---:
ALL        | 30,607,716
TRAIN      | 30,301,028
TEST       |    306,688


### Urls
 * http://www.statmt.org/lm-benchmark/

### Supervised keys (for `as_supervised=True`)
(u'text', u'text')

### Citation
```
@article{DBLP:journals/corr/ChelbaMSGBK13,
  author    = {Ciprian Chelba and
               Tomas Mikolov and
               Mike Schuster and
               Qi Ge and
               Thorsten Brants and
               Phillipp Koehn},
  title     = {One Billion Word Benchmark for Measuring Progress in Statistical Language
               Modeling},
  journal   = {CoRR},
  volume    = {abs/1312.3005},
  year      = {2013},
  url       = {http://arxiv.org/abs/1312.3005},
  archivePrefix = {arXiv},
  eprint    = {1312.3005},
  timestamp = {Mon, 13 Aug 2018 16:46:16 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/ChelbaMSGBK13},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

```

---

## `"squad"`

Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.


* URL: https://rajpurkar.github.io/SQuAD-explorer/
* `DatasetBuilder`: [`tfds.text.squad.Squad`](https://github.com/tensorflow/datasets/tree/master/tensorflow_datasets/text/squad.py)

`squad` is configured with `tfds.text.squad.SquadConfig` and has the following
configurations predefined (defaults to the first one):

* `"plain_text"` (`v0.0.1`): Plain text

* `"bytes"` (`v0.0.1`): Uses byte-level text encoding with `tfds.features.text.ByteTextEncoder`

* `"subwords8k"` (`v0.0.1`): Uses `tfds.features.text.SubwordTextEncoder` with 8k vocab size

* `"subwords32k"` (`v0.0.1`): Uses `tfds.features.text.SubwordTextEncoder` with 32k vocab size


### `"squad/plain_text"`

```
FeaturesDict({
    'context': Text(shape=(), dtype=tf.string, encoder=None),
    'first_answer': Text(shape=(), dtype=tf.string, encoder=None),
    'question': Text(shape=(), dtype=tf.string, encoder=None),
})
```



### `"squad/bytes"`

```
FeaturesDict({
    'context': Text(shape=(None,), dtype=tf.int64, encoder=<ByteTextEncoder vocab_size=257>),
    'first_answer': Text(shape=(None,), dtype=tf.int64, encoder=<ByteTextEncoder vocab_size=257>),
    'question': Text(shape=(None,), dtype=tf.int64, encoder=<ByteTextEncoder vocab_size=257>),
})
```



### `"squad/subwords8k"`

```
FeaturesDict({
    'context': Text(shape=(None,), dtype=tf.int64, encoder=<SubwordTextEncoder vocab_size=8190>),
    'first_answer': Text(shape=(None,), dtype=tf.int64, encoder=<SubwordTextEncoder vocab_size=8190>),
    'question': Text(shape=(None,), dtype=tf.int64, encoder=<SubwordTextEncoder vocab_size=8190>),
})
```



### `"squad/subwords32k"`

```
FeaturesDict({
    'context': Text(shape=(None,), dtype=tf.int64, encoder=<SubwordTextEncoder vocab_size=32953>),
    'first_answer': Text(shape=(None,), dtype=tf.int64, encoder=<SubwordTextEncoder vocab_size=32953>),
    'question': Text(shape=(None,), dtype=tf.int64, encoder=<SubwordTextEncoder vocab_size=32953>),
})
```




### Statistics
Split  | Examples
:----- | ---:
ALL        |     98,169
TRAIN      |     87,599
VALIDATION |     10,570


### Urls
 * https://rajpurkar.github.io/SQuAD-explorer/

### Supervised keys (for `as_supervised=True`)
None

### Citation
```
@article{2016arXiv160605250R,
       author = {{Rajpurkar}, Pranav and {Zhang}, Jian and {Lopyrev},
                 Konstantin and {Liang}, Percy},
        title = "{SQuAD: 100,000+ Questions for Machine Comprehension of Text}",
      journal = {arXiv e-prints},
         year = 2016,
          eid = {arXiv:1606.05250},
        pages = {arXiv:1606.05250},
archivePrefix = {arXiv},
       eprint = {1606.05250},
}

```

---


# [`translate`](#translate)

## `"wmt_translate_ende"`

Translate dataset based on the data from statmt.org.


* URL: http://www.statmt.org/wmt18/
* `DatasetBuilder`: [`tfds.translate.wmt_ende.WmtTranslateEnde`](https://github.com/tensorflow/datasets/tree/master/tensorflow_datasets/translate/wmt_ende.py)

`wmt_translate_ende` is configured with `tfds.translate.wmt_ende.WMTConfig` and has the following
configurations predefined (defaults to the first one):

* `"ende_plain_text_t2t"` (`v0.0.1`): Translation dataset from en to de, uses encoder plain_text. It uses the following data files (see the code for exact contents): {"dev": ["wmt17_newstest13"], "train": ["wmt18_news_commentary_ende", "wmt13_commoncrawl_ende", "wmt13_europarl_ende"]}.

* `"ende_subwords8k_t2t"` (`v0.0.1`): Translation dataset from en to de, uses encoder subwords8k. It uses the following data files (see the code for exact contents): {"dev": ["wmt17_newstest13"], "train": ["wmt18_news_commentary_ende", "wmt13_commoncrawl_ende", "wmt13_europarl_ende"]}.


### `"wmt_translate_ende/ende_plain_text_t2t"`

```
FeaturesDict({
    'de': Text(shape=(), dtype=tf.string, encoder=None),
    'en': Text(shape=(), dtype=tf.string, encoder=None),
})
```



### `"wmt_translate_ende/ende_subwords8k_t2t"`

```
FeaturesDict({
    'de': Text(shape=(), dtype=tf.string, encoder=None),
    'en': Text(shape=(), dtype=tf.string, encoder=None),
})
```




### Statistics
None computed

### Urls
 * http://www.statmt.org/wmt18/

### Supervised keys (for `as_supervised=True`)
(u'en', u'de')

### Citation
```
@InProceedings{bojar-EtAl:2018:WMT1,
  author    = {Bojar, Ond{r}ej  and  Federmann, Christian  and  Fishel, Mark
    and Graham, Yvette  and  Haddow, Barry  and  Huck, Matthias  and
    Koehn, Philipp  and  Monz, Christof},
  title     = {Findings of the 2018 Conference on Machine Translation (WMT18)},
  booktitle = {Proceedings of the Third Conference on Machine Translation,
    Volume 2: Shared Task Papers},
  month     = {October},
  year      = {2018},
  address   = {Belgium, Brussels},
  publisher = {Association for Computational Linguistics},
  pages     = {272--307},
  url       = {http://www.aclweb.org/anthology/W18-6401}
}

```

---

## `"wmt_translate_enfr"`

Translate dataset based on the data from statmt.org.


* URL: http://www.statmt.org/wmt18/
* `DatasetBuilder`: [`tfds.translate.wmt_enfr.WmtTranslateEnfr`](https://github.com/tensorflow/datasets/tree/master/tensorflow_datasets/translate/wmt_enfr.py)

`wmt_translate_enfr` is configured with `tfds.translate.wmt_enfr.WMTConfig` and has the following
configurations predefined (defaults to the first one):

* `"enfr_plain_text_t2t_small"` (`v0.0.1`): Translation dataset from en to fr, uses encoder plain_text. It uses the following data files (see the code for exact contents): {"dev": ["opennmt_1M_enfr_valid"], "train": ["opennmt_1M_enfr_train"]}.

* `"enfr_subwords8k_t2t_small"` (`v0.0.1`): Translation dataset from en to fr, uses encoder subwords8k. It uses the following data files (see the code for exact contents): {"dev": ["opennmt_1M_enfr_valid"], "train": ["opennmt_1M_enfr_train"]}.

* `"enfr_plain_text_t2t_large"` (`v0.0.1`): Translation dataset from en to fr, uses encoder plain_text. It uses the following data files (see the code for exact contents): {"dev": ["wmt17_newstest13"], "train": ["wmt13_commoncrawl_enfr", "wmt13_europarl_enfr", "wmt14_news_commentary_enfr", "wmt13_undoc_enfr"]}.

* `"enfr_subwords8k_t2t_large"` (`v0.0.1`): Translation dataset from en to fr, uses encoder subwords8k. It uses the following data files (see the code for exact contents): {"dev": ["wmt17_newstest13"], "train": ["wmt13_commoncrawl_enfr", "wmt13_europarl_enfr", "wmt14_news_commentary_enfr", "wmt13_undoc_enfr"]}.


### `"wmt_translate_enfr/enfr_plain_text_t2t_small"`

```
FeaturesDict({
    'en': Text(shape=(), dtype=tf.string, encoder=None),
    'fr': Text(shape=(), dtype=tf.string, encoder=None),
})
```



### `"wmt_translate_enfr/enfr_subwords8k_t2t_small"`

```
FeaturesDict({
    'en': Text(shape=(), dtype=tf.string, encoder=None),
    'fr': Text(shape=(), dtype=tf.string, encoder=None),
})
```



### `"wmt_translate_enfr/enfr_plain_text_t2t_large"`

```
FeaturesDict({
    'en': Text(shape=(), dtype=tf.string, encoder=None),
    'fr': Text(shape=(), dtype=tf.string, encoder=None),
})
```



### `"wmt_translate_enfr/enfr_subwords8k_t2t_large"`

```
FeaturesDict({
    'en': Text(shape=(), dtype=tf.string, encoder=None),
    'fr': Text(shape=(), dtype=tf.string, encoder=None),
})
```




### Statistics
None computed

### Urls
 * http://www.statmt.org/wmt18/

### Supervised keys (for `as_supervised=True`)
(u'en', u'fr')

### Citation
```
@InProceedings{bojar-EtAl:2018:WMT1,
  author    = {Bojar, Ond{r}ej  and  Federmann, Christian  and  Fishel, Mark
    and Graham, Yvette  and  Haddow, Barry  and  Huck, Matthias  and
    Koehn, Philipp  and  Monz, Christof},
  title     = {Findings of the 2018 Conference on Machine Translation (WMT18)},
  booktitle = {Proceedings of the Third Conference on Machine Translation,
    Volume 2: Shared Task Papers},
  month     = {October},
  year      = {2018},
  address   = {Belgium, Brussels},
  publisher = {Association for Computational Linguistics},
  pages     = {272--307},
  url       = {http://www.aclweb.org/anthology/W18-6401}
}

```

---


# [`video`](#video)

## `"bair_robot_pushing_small"`

This data set contains roughly 59,000 examples of robot pushing motions, including one training set (train) and two test sets of previously seen (testseen) and unseen (testnovel) objects. This is the small 64x64 version.

* URL: https://sites.google.com/site/brainrobotdata/home/push-dataset
* `DatasetBuilder`: [`tfds.video.bair_robot_pushing.BairRobotPushingSmall`](https://github.com/tensorflow/datasets/tree/master/tensorflow_datasets/video/bair_robot_pushing.py)
* Version: `v1.0.0`

### Features
```
FeaturesDict({
    'action': Tensor(shape=(4,), dtype=tf.float32),
    'endeffector_pos': Tensor(shape=(3,), dtype=tf.float32),
    'image_aux1': Image(shape=(64, 64, 3), dtype=tf.uint8),
    'image_main': Image(shape=(64, 64, 3), dtype=tf.uint8),
})
```


### Statistics
Split  | Examples
:----- | ---:
ALL        |     43,520
TRAIN      |     43,264
TEST       |        256


### Urls
 * https://sites.google.com/site/brainrobotdata/home/push-dataset

### Supervised keys (for `as_supervised=True`)
None

### Citation
```
@inproceedings{conf/nips/FinnGL16,
  added-at = {2016-12-16T00:00:00.000+0100},
  author = {Finn, Chelsea and Goodfellow, Ian J. and Levine, Sergey},
  biburl = {https://www.bibsonomy.org/bibtex/230073873b4fe43b314724b772d0f9256/dblp},
  booktitle = {NIPS},
  crossref = {conf/nips/2016},
  editor = {Lee, Daniel D. and Sugiyama, Masashi and Luxburg, Ulrike V. and Guyon, Isabelle and Garnett, Roman},
  ee = {http://papers.nips.cc/paper/6161-unsupervised-learning-for-physical-interaction-through-video-prediction},
  interhash = {2e6b416723704f4aa5ad0686ce5a3593},
  intrahash = {30073873b4fe43b314724b772d0f9256},
  keywords = {dblp},
  pages = {64-72},
  timestamp = {2016-12-17T11:33:40.000+0100},
  title = {Unsupervised Learning for Physical Interaction through Video Prediction.},
  url = {http://dblp.uni-trier.de/db/conf/nips/nips2016.html#FinnGL16},
  year = 2016
}

```

---

## `"starcraft_video"`

This data set contains videos generated from Starcraft.

* URL: https://storage.googleapis.com/scv_dataset/README.html
* `DatasetBuilder`: [`tfds.video.starcraft.StarcraftVideo`](https://github.com/tensorflow/datasets/tree/master/tensorflow_datasets/video/starcraft.py)

`starcraft_video` is configured with `tfds.video.starcraft.StarcraftVideoConfig` and has the following
configurations predefined (defaults to the first one):

* `"brawl_64"` (`v0.1.1`): Brawl map with 64x64 resolution.

* `"brawl_128"` (`v0.1.1`): Brawl map with 128x128 resolution.

* `"collect_mineral_shards_64"` (`v0.1.1`): CollectMineralShards map with 64x64 resolution.

* `"collect_mineral_shards_128"` (`v0.1.1`): CollectMineralShards map with 128x128 resolution.

* `"move_unit_to_border_64"` (`v0.1.1`): MoveUnitToBorder map with 64x64 resolution.

* `"move_unit_to_border_128"` (`v0.1.1`): MoveUnitToBorder map with 128x128 resolution.

* `"road_trip_with_medivac_64"` (`v0.1.1`): RoadTripWithMedivac map with 64x64 resolution.

* `"road_trip_with_medivac_128"` (`v0.1.1`): RoadTripWithMedivac map with 128x128 resolution.


### `"starcraft_video/brawl_64"`

```
FeaturesDict({
    'rgb_screen': Video(shape=(None, 64, 64, 3), dtype=tf.uint8, feature=Image(shape=(64, 64, 3), dtype=tf.uint8)),
})
```



### `"starcraft_video/brawl_128"`

```
FeaturesDict({
    'rgb_screen': Video(shape=(None, 128, 128, 3), dtype=tf.uint8, feature=Image(shape=(128, 128, 3), dtype=tf.uint8)),
})
```



### `"starcraft_video/collect_mineral_shards_64"`

```
FeaturesDict({
    'rgb_screen': Video(shape=(None, 64, 64, 3), dtype=tf.uint8, feature=Image(shape=(64, 64, 3), dtype=tf.uint8)),
})
```



### `"starcraft_video/collect_mineral_shards_128"`

```
FeaturesDict({
    'rgb_screen': Video(shape=(None, 128, 128, 3), dtype=tf.uint8, feature=Image(shape=(128, 128, 3), dtype=tf.uint8)),
})
```



### `"starcraft_video/move_unit_to_border_64"`

```
FeaturesDict({
    'rgb_screen': Video(shape=(None, 64, 64, 3), dtype=tf.uint8, feature=Image(shape=(64, 64, 3), dtype=tf.uint8)),
})
```



### `"starcraft_video/move_unit_to_border_128"`

```
FeaturesDict({
    'rgb_screen': Video(shape=(None, 128, 128, 3), dtype=tf.uint8, feature=Image(shape=(128, 128, 3), dtype=tf.uint8)),
})
```



### `"starcraft_video/road_trip_with_medivac_64"`

```
FeaturesDict({
    'rgb_screen': Video(shape=(None, 64, 64, 3), dtype=tf.uint8, feature=Image(shape=(64, 64, 3), dtype=tf.uint8)),
})
```



### `"starcraft_video/road_trip_with_medivac_128"`

```
FeaturesDict({
    'rgb_screen': Video(shape=(None, 128, 128, 3), dtype=tf.uint8, feature=Image(shape=(128, 128, 3), dtype=tf.uint8)),
})
```




### Statistics
Split  | Examples
:----- | ---:
ALL        |     14,000
TRAIN      |     10,000
VALIDATION |      2,000
TEST       |      2,000


### Urls
 * https://storage.googleapis.com/scv_dataset/README.html

### Supervised keys (for `as_supervised=True`)
None

### Citation
```
Towards Accurate Generative Models of Video: New Metrics & Challenges
```

---


